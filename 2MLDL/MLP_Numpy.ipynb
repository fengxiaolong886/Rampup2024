{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4352ec20-61ae-4552-ae1a-88f47509ed95",
   "metadata": {},
   "source": [
    "# MLP Implementation by Numpy\n",
    "Use MLP to slove the MNIST problem.\n",
    "- use dense layers. f(x) = wx + b\n",
    "- use ReLu\n",
    "- use crossentropy loss function\n",
    "- Apply backprop algrithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3493edd-0b41-45d4-ac2b-ca8dc812b784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 15:21:42.325910: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-12 15:21:42.453847: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-12 15:21:44.786486: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-12 15:21:44.989663: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-12 15:21:45.172523: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-12 15:21:45.221179: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-12 15:21:45.544598: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-12 15:21:49.411351: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import datasets\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a3ba4e3-d992-4000-b74c-416836304c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define layer template\n",
    "class Layer():\n",
    "    \"\"\"Define layers building block. There is two main function.\n",
    "    -  Process the input to get the output:    output = layer.forward(input)\n",
    "    -  Propagate gradients through itself.     grad_input = layer.backward(input, grad_output)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Here initialize the layer parameters\"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Input date with shape [batch, input_units], returns the output data [batch, output_units]\"\"\"\n",
    "        return input\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"Perform the backpropogradtion step through the layer, with respect to the given input.\n",
    "        Compute the loss gradients w.r.t input, and apply chain rule for backprop:\n",
    "        d loss / dx = (d loss / d layer) * (d layer / dx)\n",
    "        For this function, already receive (d loss / d layer),  so only need to multiply it by (d layer / dx)\n",
    "        \"\"\"\n",
    "        num_units = input.shape[1]\n",
    "        d_layer_d_input = np.eye(num_units)\n",
    "        return np.dot(grad_output, d_layer_d_input)\n",
    "\n",
    "# Define ReLu\n",
    "class ReLu(Layer):\n",
    "    def __init__(self):\n",
    "        \"\"\" ReLu layer simply apply the elementwise recificed linear unit to all input.\"\"\"\n",
    "        \"\"\" y = max(0, x)\"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Here apply elementwise ReLu to [batch, input_units] matrix\"\"\"\n",
    "        return np.maximum(0, input)\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\" Compute the gradient of loss w.r.t ReLu input\"\"\"\n",
    "        relu_grad = input > 0 \n",
    "        return grad_output * relu_grad\n",
    "\n",
    "# Define Dense layer\n",
    "class Dense(Layer):\n",
    "    def  __init__(self, input_units, output_units, learning_rate = 0.1):\n",
    "        \"\"\"Perform the function f(x) = Wx + B\"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        # initialize the weights with normal initializeation.\n",
    "        self.weights = np.random.randn(input_units, output_units) * 0.01\n",
    "        self.biases = np.zeros(output_units)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Perform  f(x) = Wx + b\n",
    "        input shape:  [batch, input_units]\n",
    "        output shape: [batch, output_units]\n",
    "        \"\"\"\n",
    "        return np.dot(input, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        # Here is df/dx = df/d dense  * d dense/dx,  w.r.t d dense/dx = weights transposed\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "\n",
    "        # compute the gradient w.r.t. weights and biases\n",
    "        grad_weights = np.dot(input.T, grad_output) / input.shape[0]\n",
    "        grad_biases = grad_output.mean(axis = 0)\n",
    "\n",
    "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
    "\n",
    "        # define the stochastic gradient descent step.\n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4747969-c695-49f0-83d6-4fef3e03fe05",
   "metadata": {},
   "source": [
    "####  Define the loss function\n",
    "\n",
    "As we use the softmax, the loass as:\n",
    "\n",
    "$ loss = - log \\space {e^{a_{correct}} \\over {\\underset i \\sum e^{a_i} } } $\n",
    "\n",
    "rewritten as:\n",
    "\n",
    "$ loss = - a_{correct} + log {\\underset i \\sum e^{a_i} } $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b50530b4-2468-4c8a-81d9-d718a6a0a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits, reference_answer):\n",
    "    \"\"\"Compute the crossentropy from logits  [batch, n_classes] and ids of correct answers\"\"\"\n",
    "    logits_for_answers = logits[np.arange(len(logits)), reference_answer]\n",
    "\n",
    "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits), axis = -1))\n",
    "\n",
    "    return xentropy\n",
    "\n",
    "def grad_softmax_crossentropy_with_logits(logits, reference_answer):\n",
    "    \"\"\"compute the crossentropy gradient from logits [batch, n_classes] and ids of correct answers\"\"\"\n",
    "    ones_for_answers = np.zeros_like(logits)\n",
    "\n",
    "    ones_for_answers[np.arange(len(logits)), reference_answer] = 1\n",
    "\n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis = -1, keepdims = True)\n",
    "\n",
    "    return - ones_for_answers + softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a65136a-d3fa-4547-8a3a-6e69d81eb8ff",
   "metadata": {},
   "source": [
    "### Load MNIST test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9bbbbf2-e7ca-4666-8edb-a138b7e2fe1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28)\n",
      "train samples: 60000\n",
      "test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# load test data and split it as train set and test set.\n",
    "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "\n",
    "#Scale image to [0, 1] for all input x data\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "## make sure images have the same input dimension with (28, 28, 1)\n",
    "#x_train = np.expand_dims(x_train, -1)\n",
    "#x_test = np.expand_dims(x_train, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"train samples:\", x_train.shape[0])\n",
    "print(\"test samples:\", x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37270697-32c4-4079-a8f1-7493a45be745",
   "metadata": {},
   "source": [
    "### Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d68cb0f7-3730-4ea6-93d6-404d0d5947f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = []\n",
    "network.append(Dense(x_train.shape[1], 100))\n",
    "network.append(ReLu())\n",
    "network.append(Dense(100, 200))\n",
    "network.append(ReLu())\n",
    "network.append(Dense(200, 10))\n",
    "\n",
    "# Forward Path\n",
    "def forward(network, X):\n",
    "    activations = []\n",
    "    input = X\n",
    "\n",
    "    for layer in network:\n",
    "        activations.append(layer.forward(input))\n",
    "        input = activations[-1]\n",
    "\n",
    "    assert len(activations) == len(network)\n",
    "    return activations\n",
    "\n",
    "def predict(network, X):\n",
    "    logits = forward(network, X)[-1]\n",
    "    return logits.argmax(axis = -1)\n",
    "\n",
    "def train(network, X, y):\n",
    "    \"\"\"\n",
    "    1. run forward to get all layer activations\n",
    "    2. run layer.backward grom last to the first layer.\n",
    "    \"\"\"\n",
    "    # calcuate the activations\n",
    "    layer_activations = forward(network, X)\n",
    "    layer_inputs = [X] + layer_activations\n",
    "    logits = layer_activations[-1]\n",
    "\n",
    "    # Compute the loss and initial gradient\n",
    "    loss = softmax_crossentropy_with_logits(logits, y)\n",
    "    loss_grad = grad_softmax_crossentropy_with_logits(logits, y)\n",
    "\n",
    "    # propagate gradients through the network\n",
    "    for layer_i in range(len(network))[::-1]:\n",
    "        layer = network[layer_i]\n",
    "\n",
    "        loss_grad = layer.backward(layer_inputs[layer_i], loss_grad) # gradient w.r.t input and weight\n",
    "\n",
    "    return np.mean(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f760f612-86be-4781-a1f4-bf84b01eba16",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c214880f-5719-40af-860c-a8d02fb6cd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (8,10) (8,28) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m25\u001b[39m):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m iternate_minibatches(x_train, y_train, batchsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 20\u001b[0m         \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     train_log\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(predict(network, x_train) \u001b[38;5;241m==\u001b[39m y_train))\n\u001b[1;32m     23\u001b[0m     val_log\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(predict(network, x_test) \u001b[38;5;241m==\u001b[39m y_test))\n",
      "Cell \u001b[0;32mIn[5], line 35\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(network, X, y)\u001b[0m\n\u001b[1;32m     32\u001b[0m logits \u001b[38;5;241m=\u001b[39m layer_activations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Compute the loss and initial gradient\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax_crossentropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m loss_grad \u001b[38;5;241m=\u001b[39m grad_softmax_crossentropy_with_logits(logits, y)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# propagate gradients through the network\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m, in \u001b[0;36msoftmax_crossentropy_with_logits\u001b[0;34m(logits, reference_answer)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the crossentropy from logits  [batch, n_classes] and ids of correct answers\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m logits_for_answers \u001b[38;5;241m=\u001b[39m logits[np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(logits)), reference_answer]\n\u001b[0;32m----> 5\u001b[0m xentropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogits_for_answers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xentropy\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (8,10) (8,28) "
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "def iternate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "\n",
    "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx: start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "from IPython.display import clear_output\n",
    "train_log = []\n",
    "val_log = []\n",
    "\n",
    "for epoch in range(25):\n",
    "    for x_batch, y_batch in iternate_minibatches(x_train, y_train, batchsize=8, shuffle= True):\n",
    "        train(network, x_batch, y_batch)\n",
    "\n",
    "    train_log.append(np.mean(predict(network, x_train) == y_train))\n",
    "    val_log.append(np.mean(predict(network, x_test) == y_test))\n",
    "\n",
    "    clear_output()\n",
    "    print(\"Epoch:\", epoch)\n",
    "    print(\"Train accuracy: \", train_log[-1])\n",
    "    print(\"Test accuracy: \", val_log[-1])\n",
    "    plt.plot(train_log, label = \"Train accuracy\")\n",
    "    plt.plot(val_log, label = \"Test accuracy\")\n",
    "    plt.legend(loc = \"best\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7300e7-ac7f-4a62-bf3d-3effd35e4f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdb1191-b829-40f7-b52d-a55448ddf53f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aeca0c10-c0a0-4ead-99c7-7283824c0d1b",
   "metadata": {},
   "source": [
    "# Unit test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091a7373-5d96-4527-9587-d8251487f2c6",
   "metadata": {},
   "source": [
    "### Test code for loss function(not completed version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68da45df-54a5-46bf-8ceb-55f8be1f1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = np.linspace(-1,1,500).reshape([50,10])\n",
    "answers = np.arange(50)%10\n",
    "\n",
    "softmax_crossentropy_with_logits(logits,answers)\n",
    "grads = grad_softmax_crossentropy_with_logits(logits,answers)\n",
    "#numeric_grads = eval_numerical_gradient(lambda l: softmax_crossentropy_with_logits(l,answers).sum(),logits)\n",
    "#assert np.allclose(numeric_grads,grads,rtol=1e-3,atol=0), \"omfg, reference implementation just failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a5d733-a4bd-4e3d-a2bf-803ba69cfab2",
   "metadata": {},
   "source": [
    "### Test code for Dense layer\n",
    "Target: perform a few test to make sure dense layer works properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bfec63-3006-4f87-9a29-bdc5a7e61159",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Dense(128,150)\n",
    "\n",
    "assert -0.05 < l.weights.mean() < 0.05 and 1e-3 < l.weights.std() < 1e-1, \"weights must be zero mean and have small variance.\"\\\n",
    "                                                                  \"If you know what you're doing, remove this assertion.\"\n",
    "assert -0.05 < l.biases.mean() < 0.05, \"Biases must be zero mean. Ignore if you have a reason to do otherwise.\"\n",
    "\n",
    "#to test outputs, we explicitly set weights with fixed values. DO NOT DO THAT IN ACTUAL NETWORK!\n",
    "l = Dense(3,4)\n",
    "\n",
    "x = np.linspace(-1,1,2*3).reshape([2,3])\n",
    "l.weights = np.linspace(-1,1,3*4).reshape([3,4])\n",
    "l.biases = np.linspace(-1,1,4)\n",
    "\n",
    "assert np.allclose(l.forward(x),np.array([[ 0.07272727,  0.41212121,  0.75151515,  1.09090909],\n",
    "                                          [-0.90909091,  0.08484848,  1.07878788,  2.07272727]]))\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccaa747-ad59-49cd-90fc-846ce91f221d",
   "metadata": {},
   "source": [
    "### Check training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f567cc4-4ea8-4470-99c5-b7ca5b595df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize = [6, 6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.title(\"Label: %i\"%y_train[i])\n",
    "    plt.imshow(x_train[i].reshape([28, 28]), cmap = \"gray\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
